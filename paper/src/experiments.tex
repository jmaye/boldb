In order to evaluate the approach proposed in this paper, we have collected
a dataset with a car in an urban setting. Our car is equipped with a Sony
XCD-SX910 camera recording $1280$x$960$ images at $3.75$ frames per second and
an XSens MTi IMU running at $100$ Hz with $x$ pointing forward, $y$ to the
right, and $z$ upward. The sequence contains $8218$ images and lasts around $40$
minutes. We have encountered different scenes comprising of traffic lights,
crosswalks, or changes of speed limit. We have driven in a loop so as to come
several times in the same situation and thus have an estimation of the quality
of our solution.

In the rest of this Section, we first display an experiment of the whole
algorithm on synthetic data. It is easier to have a ground truth on simulated
data and hence validate our approach. We then show results on real-world data
with a detailed analysis of all the components.

\subsection{Simulation}
For visualization purposes, we have simulated IMU data with an univariate normal
distribution and have introduced change-points every $50$ data points. We have
randomly generated 3 different $\boldsymbol{\alpha}_i$ with $K=256$ coding for
the traffic situations. Although the algorithm starts with no prior knowledge,
we could also start with previously learned models $M_i$ and $A_i$.

Fig.~\ref{fig:simulation} depicts the output of the simulation and demonstrates
the pertinence of our method. After a change-point, the algorithm needs a time
of adaptation to collect enough data points in order to converge to the correct
prediction. In Fig.~\ref{fig:simulation}, we display the MAP solution for
\eqref{eqn:action} on the bottom plot and thus the prediction reflects the
Gaussian component with the biggest number of data points. The code runs in
MATLAB with no attempt for optimization and has a processing time of about
$0.3$ seconds per time step. In a realistic scenario, histogram building from
image adds up to the time.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/simResult.eps}
\caption{Simulation results of the entire algorithm. From top to bottom, the
plots display the simulated IMU data $\mathbf{z}_t$, the inferred segment
lengths $r_t$, the inferred labels $l_t$ (blue circle) and ground truth
(red square), and the MAP for $\mathbf{a}_t$.}
\label{fig:simulation}
\end{figure}

\subsection{Motion Segmentation}
In this experiment, we want to estimate the quality of our motion segmentation
algorithm from Section~\ref{sec:motion}. We performed inference on the
final posterior distribution \eqref{eqn:joint} to get the optimal
sequence of segment lengths which represents our motion segments. We set the
hazard rate to $\lambda=1/10$, the number of particles to $P=100$, and the prior
hyperparameters of the normal-Wishart to $\kappa_0=1,
\boldsymbol{\rho}_0=\mathbf{0},\nu_0=3,\boldsymbol{\Lambda}_0=\mathbf{I}$. We
only considered IMU data at $10$ Hz.

Fig.~\ref{fig:motion_segments} shows the extracted motion segments along with
the corresponding IMU data. Our algorithm identified $165$ segments which are
validated by visual inspection of the IMU data. Furthermore, the segmentation
has been compared to a manual annotation of our image sequence and exhibited an
accuracy of approximatively $92\%$. For the labeling of the change-points, we
have watched the video and noted down where we would expect a change of driving
behavior. The parameter $\lambda$ controls the false positives/negatives rates.
As expected, the typical motion patterns that appeared were driving straight
with no acceleration, driving straight with acceleration or deceleration, and
turning left or right.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/cpResult.eps}
\caption{Optimal motion segmentation from IMU data. The three top plots are the
IMU raw values over time. The bottom plot depicts the motion segments
discovered by our algorithm.}
\label{fig:motion_segments}
\end{figure}

\subsection{Traffic Situation Labeling and Recognition}
In this experiment, we are interested in evaluating the part of our algorithm
presented in Section~\ref{sec:labeling} and perform inference on
\eqref{eqn:labeling}. In a first phase, we collected a subset
of images from traffic lights, yield signs, pedestrian crossings, and regular
straight roads. Models $M_i$ were learned on these images and frozen during the
evaluation. In a second phase, we started the algorithm with no prior models.
The dictionary was created from a set of $N=400$ randomly picked images and the
SIFT features quantized into $K=256$ visual words.

ANOTHER PARAGRAPH SHOWING AND EXPLAINING THE RESULTS. IF SPACE, SHOW A GRAPH.

\subsection{Action Prediction}
FIRST PARAGRAPH DESCRIBING THE EXPERIMENTAL CONDITIONS.

SECOND PARAGRAPH OUTPUTTING AND COMMENTING THE RESULTS.
